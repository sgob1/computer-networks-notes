\documentclass[10pt]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[document]{ragged2e}
\usepackage{microtype}
\usepackage{notomath}
\usepackage{CharisSIL}
\usepackage[all]{nowidow}
\usepackage{graphicx}
\usepackage{svg}
\usepackage[pdfa]{hyperref}
\usepackage{color}
\usepackage{setspace}
\usepackage{parskip}
\usepackage[a4paper, inner=4.0cm, outer=5.0cm]{geometry}
\usepackage{listings}
\definecolor{dkgreen}{rgb}{0.1,0.5,0.1}
\definecolor{greengray}{rgb}{0.517,0.761,0.404}
\definecolor{orange}{rgb}{0.717,0.274,0.105}
\definecolor{blue}{rgb}{0.164,0.317,0.600}
\definecolor{background}{rgb}{0.990,0.990,0.990}
\lstset {
	frame=lrtb,
	language=java,
	aboveskip=0.7cm,
	belowskip=0.2cm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	backgroundcolor=\color{background},
	numberstyle=\tiny\color{dkgreen},
	keywordstyle=\color{blue},
	commentstyle=\color{greengray},
	stringstyle=\color{orange},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}

\newtheorem{thm}{Teorema}
\setlength{\tabcolsep}{0.5em} % for the horizontal padding
{\renewcommand{\arraystretch}{1.35}% for the vertical padding

\usepackage{fancyhdr}


\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{\leftmark}
%\fancyhead[RE,LO]{Programmazione Avanzata}
%\fancyfoot[CE,CO]{\leftmark}
\fancyfoot[LE,RO]{\thepage}

\renewcommand{\headrulewidth}{1pt}
%\renewcommand{\footrulewidth}{1pt}

\usepackage{titlesec}
\titleformat{\part}[display]
{\Huge\bfseries}{}{10pt}{\center{ Parte \thepart\ \ \\ \ \ }}
%\titleformat{\chapter}[display]
%{\huge\bfseries}{}{0pt}{\thechapter\ \ \--\ \ }
\titleformat{\section}[display]
{\Large\bfseries}{}{0pt}{\medskip\thesection\ \ \--\ \ }
\titleformat{\subsection}[display]
{\large\bfseries}{}{0pt}{\medskip\thesubsection\ \ \--\ \ }

\begin{document}
\setmonofont[Scale=.85]{Fira Code Retina}


\include{cn-titlepage}
\setstretch{1.35}
\setlength{\headsep}{30pt}
\setlength{\footskip}{40pt}
\setlength{\marginparsep}{20pt}
\setlength{\marginparwidth}{60pt}
%\title{Computer Networks 2 and Introduction to Cybersecurity}
%\author{Marco Sgobino}
%\maketitle
\tableofcontents

\part{Transmission Control Protocol internals}

\chapter{TCP Workings}

\section{Recap}
TCP is a procotol that has the following remarkable properties,

\begin{enumerate}
	\item is \emph{connection-oriented}: before transmitting data, a
		connection must be established \--- this is different to what
		IP protocol does in network layer, since in IP protocol there
		is no concept of connection, and messages are exchanged in
		packets;
	\item is \emph{reliable}: it assures all segments are correctly
		delivered through use of \emph{ACK} mechanism, and \textbf{at
		most once} \--- again, this is different to IP layer, since IP
		packets are unreliable and there is no guarantee that they will
		be delivered, let alone in their correct ordering;
	\item offers a \emph{sliding window} mechanism for congestion control
		and stream control. This assures read and send buffers are
		well-optimized in both sender and receiver \--- this feature is
		much important since it allows fine-tuning the connection and
		avoiding waste of resources;
	\item is \emph{byte-oriented}: the byte stream is fragmented into
		multiple segments, and composed again after getting to
		destination \--- in this manner, huge payloads can be reliably
		delivered in multiple segments, each one having the correct
		order to the application. Application has the necessary information
		to easily reconstruct the correct ordering of the acquired 
		information.
\end{enumerate}

Ultimately, TCP allows creating connections between application processes,
offering multiple services to the upper layers.

\subsection{Sockets}

The TCP makes use of \emph{sockets} abstraction as a way of serving data to the
above application layer. The logical structure is the following one: there are
two entities, client and server. The client first authenticates to the server;
after that, the server opens a connection and client executes the send-receive
loop. Both server and client create a \emph{socket} $s$ (a UNIX-like file
abstraction), and the client connect $s$ to \texttt{IP-srv}, \texttt{port-srv}.
Each socket is then binded to a specific IP address and a specific \emph{port
number}.

The communication takes place on $s$ by means of application protocol, with
applications exchanging data each other. The connection is said to be
\emph{reliable}: losses and packet loss are carefully managed by the TCP protocol, and
segments\footnote{\emph{Segments} refer to the particular kind of package that
TCP protocol exchange. Usually, in literature one could find the term package
referring to TCP segments; while this is not fully correct, it may be
acceptable when there is no ambiguity.} are collected in the very same order as
they are sent.

A very simplified pseudo-code at client's side for TCP is as follows,

\begin{verbatim} int s; s := socket(...); 
connect(s, IP-srv, port-srv,...); 
...
send (s, msgl, ...); 
... 
msg2 := receive(s,...); 
... 
\end{verbatim}

where the notation \emph{...} denotes possibly code in between two
instructions. In the above code, a client wants to send data to a remove server
having a specific IP address and a port number. The client first creates a
socket, then connects it to the server IP address and to its port number. After
connection has been established, the client can subsequently invoke
\texttt{send()} to command TCP layer to deliver desired data to the other end.
To the very same socket, data can be received as well: by invoking
\texttt{receive()} the client can successfully retrieve eventual data sent by
the server.

At server side, the logical structure is quite different. A server creates
a socket \texttt{s1}, chooses a port number to bind to that socket (usually a
standard one), then it declares \emph{willingness to accept connections} on
\texttt{s1}, and finally it awaits for connection requests on \texttt{s1}. Upon
receiving a connection request, the server handles it by creating
\emph{another} socket \texttt{s2} and managing the connection on this newly
generated socket, this time with a different port number.  Basically, it
creates two different sockets, the first to accept a new connection, and the
second one to actually manage the connection. A server usually remains on
\emph{sleep} until a connection is requested.

\begin{verbatim} 
s1 := socket(...);
bind(s1, portsrvm ...);
listen(s1,...);
s2 := accept(s1,...); // another socket 
... 
msg1 := receive(s2,...);
...
send(s2,msg2,...);
... 
\end{verbatim}

Details of the above code largely depend on the platform on which the server is
operating.

\section{TCP Implementation}

In TCP, communication is \emph{bidirectional}, with a pattern that depends on
the application protocol. The send-receive patterns heavily depend on the
application itself \--- browser-related send\----receive sequences are very
different from, let's say, an e-mail client send-receive sequence. There is no
guarantee that two different application protocols will exchange a similar
amount and kind of TCP messages, let alone the very same messages.

TCP layer is built on top of the IP layer. Since there are many differences
between TCP and IP, set aside their abstraction layer, TCP should be properly
built to manage IP differences and quirks. 

IP protocol operates between \emph{nodes}: it is \emph{connectionless},
\textbf{unreliable}, and is \emph{message-oriented}. The \textbf{Maximum Transmission
Unit} size of an IP packet is $MTU = 64KB$: this means a TCP segment can never
be greater than the MTU.

TCP layers communicate between themselves in terms of \emph{segments}. A
segment is a single \emph{message between TCP layers}, and contains a \emph{TCP
header} and \--- eventually \--- data, that is said to be \emph{payload}. The
difference between information in header and information in payload is that the
first one is required by the TCP layer, while the second one is required by the
application layer, and it may be absent in some segments.

Payload in fact can either be $0$ bytes long or carry some information (wanted by the
application layer). \emph{A TCP segment must be
small enough to fit in a single IP packet}, hence IP header and TCP segment
size should be no greater than $64KB$, the MTU of an IP packet.

Since a packet is composed by a IP header, whose payload is a TCP segment, and the
TCP segment is in turn composed by a TCP header, followed by eventual
application data, the shape of a packet can be summarized as follows,

\begin{verbatim}
| IP header | TCP header |  Payload ***  |
\end{verbatim}

Usually, IP header size is usually $20$ bytes, as well as TCP
header that is $20$ bytes. The IP datagram can be greater up to $64KB$, with
the first $40$ to $50$ bytes reserved to headers.

\begin{verbatim}
| IP header | TCP header |  Payload ***  |
   20 byte     20 byte        greater
\end{verbatim}

Segments can carry portions of the original data (for instance, in a video
stream many segments should be sent to client in order to carry enough
information and let application layer reconstruct the video correctly). For
this reason, in application layer, one application message could correspond to
\emph{many segments} in TCP layer, in \textbf{both} directions. In fact, at TCP
level multiple segments are usually required in order to send a single
application-level message.

Basically, this means that while for the application layer a single
\texttt{send()} may suffice to send an entire file or all the information
required, from the TCP layer's point of view many and many segments may be
required, many more than the number of \texttt{send()} commands invoked by the
application.

\subsection{The connection state}

TCP takes care of creating \emph{connections} between processes. Connections do
have their own \emph{state}, which fundamentally univocally describes a
connection. It is enough to represent each connection with their \texttt{<id>}
and \texttt{<state>}, with the \texttt{<id>} field that contains essentially
$4$ informations,
\begin{enumerate}
    \item the \texttt{local IP} address;
    \item the \texttt{local port} number;
    \item the \texttt{remote IP} address;
    \item the \texttt{remote port} number.
\end{enumerate}

Each connection is then associated to a \texttt{state}, that serves as a
description of the actual state in which the connection lies.

IP addresses are extracted from the IP header, while port numbers are extracted
from TCP header, thus it suffices to look at the IP packet plus the TCP header.
Packets are thus managed accordingly. The connection <state> also includes
information on the \textbf{Maximum Segment Size} (MSS), which is the maximum size
of the \emph{data part} of a segment that the other part is willing to achieve.
The MSS is negotiated upon connection opening \--- this value is, in practice,
identical in both direction and \textbf{is not arbitrary}. In most cases and for
historical reasons, there are only $2$ possible values that depend whether the
connection:

\begin{itemize}
    \item \emph{lies on different networks} (through internet), MSS is $536$
        bytes (MTU=576), that is the maximum segment size that can fit in the
        smallest possible packet \-- historically this was the most reliable
        option;
    \item \emph{lies on the same network} (for example, ethernet), MSS is
        $1460$ bytes (MTU=1500), which corresponds to ethernet MTU minus the IP
        header and TCP header \-- this is a good choice in order to fit to a
        single ethernet frame.
\end{itemize}

The core idea is that each segment must be \emph{sufficiently small} to fit in
one packet along the full path, \emph{in order to prevent fragmentation}. In
fact, by transmitting larger segments one could end up with fragmented
segments, with no real advantage and many disadvantages.

\subsection{IP and TCP header structure}

\begin{figure}[h]
    \centering
    \includegraphics[ width=1.2\linewidth, height=\textheight, keepaspectratio]{./pics/tcp/ipHeader.png}
    \caption{Header of an IP packet. Source: Wikipedia.}
    \label{fig:ipHeader}
\end{figure}

Figure~\ref{fig:ipHeader} shows the structure of an IP packet. Essentially,
\begin{description}
    \item[Version, 4 bit] The first header field in an IP packet is the four-bit version field.
        For IPv4, this is always equal to $4$;
    \item[Internet Header Length (IHL), 4 bit] The IPv4 header is variable in size due to
        the optional 14th field (options). The IHL field contains the size of
        the IPv4 header, it has 4 bit that specify the number of 32-bit words
        in the header. The minimum value for this field is 5, which
        indicates a length of $5 \times 32 b = 160 b = 20 B$. As a 4-bit
        field, the maximum value is 15, this means that the maximum size of the
        IPv4 header is $15 \times 32 \mbox{ bit } = 480 \mbox{ bit } = 60 \mbox{ bytes }$.;
    \item[Differentiated Services Code Point (DSCP), 6 bit] Originally defined as the
        type of service (ToS), this field specifies differentiated services
        (DiffServ) per \texttt{RFC 2474}. Real-time data streaming makes use of the
        DSCP field. An example is Voice over IP (VoIP), which is used for
        interactive voice services;
    \item[Explicit Congestion Notification (ECN), 2 bit] This field is defined in \texttt{RFC
        3168} and allows end-to-end notification of network congestion without
        dropping packets. ECN is an optional feature available when both
        endpoints support it and effective when also supported by the
        underlying network;
    \item[Total Length, 16 bit] This 16-bit field defines the entire packet size in
        bytes, including header and data. The minimum size is 20 bytes (header
        without data) and the maximum is $65535 B$. All hosts are required
        to be able to reassemble datagrams of size up to $576 B$, but most
        modern hosts handle much larger packets. Links may impose further
        restrictions on the packet size, in which case datagrams must be
        fragmented. Fragmentation in IPv4 is performed in either the sending
        host or in routers. Reassembly is performed at the receiving host;
    \item[Identification, 16 bit] This field is an identification field and is
        primarily used for uniquely identifying the group of fragments of a
        single IP datagram. Some experimental work has suggested using the ID
        field for other purposes, such as for adding packet-tracing information
        to help trace datagrams with spoofed source addresses, but \texttt{RFC 6864}
        now prohibit any such use;
    \item[Flags, 3 bit] A three-bit field follows and is used to control or
        identify fragments. They are (in order, from most significant to least
        significant):
        \begin{itemize}
            \item bit 0: Reserved; must be zero;
            \item bit 1: Don't Fragment (DF);
            \item bit 2: More Fragments (MF).
        \end{itemize}
        If the DF flag is set, and fragmentation is required to route the packet,
        then the packet is dropped. This can be used when sending packets to a host
        that does not have resources to perform reassembly of fragments. It can
        also be used for path MTU discovery, either automatically by the host IP
        software, or manually using diagnostic tools such as ping or traceroute.
        For unfragmented packets, the MF flag is cleared. For fragmented packets,
        all fragments except the last have the MF flag set. The last fragment has a
        non-zero Fragment Offset field, differentiating it from an unfragmented
        packet;
    \item[Fragment offset, 13 bit] This field specifies the offset of a particular
        fragment relative to the beginning of the original unfragmented IP
        datagram in units of eight-byte blocks. The first fragment has an
        offset of zero. The $13$ bit field allows a maximum offset of $(213 –
        1)\times 8 = 65528 B$, which, with the header length included $(65528 +
        20 = 65548 B)$, supports fragmentation of packets exceeding the maximum
        IP length of $65535 B$;
    \item[Time to live (TTL), 8 bit] An eight-bit time to live field limits a
        datagram's lifetime to prevent network failure in the event of a
        routing loop. It is specified in seconds, but time intervals less than
        1 second are rounded up to 1. In practice, the field is used as a hop
        count—when the datagram arrives at a router, the router decrements the
        TTL field by one. When the TTL field hits zero, the router discards the
        packet and typically sends an ICMP time exceeded message to the
        sender. The program traceroute sends messages with adjusted TTL values
        and uses these ICMP time exceeded messages to identify the routers
        traversed by packets from the source to the destination;
    \item[Protocol, 8 bit] This field defines the protocol used in the data portion of
        the IP datagram. IANA maintains a list of IP protocol numbers as
        directed by \texttt{RFC 790};
    \item[Header checksum, 16 bit] The 16-bit IPv4 header checksum field is used
        for error-checking of the header. When a packet arrives at a router,
        the router calculates the checksum of the header and compares it to the
        checksum field. If the values do not match, the router discards the
        packet. Errors in the data field must be handled by the encapsulated
        protocol. Both UDP and TCP have separate checksums that apply to their
        data. When a packet arrives at a router, the router decreases the TTL
        field in the header. Consequently, the router must calculate a new
        header checksum;
    \item[Source address, 32 bit] This field is the IPv4 address of the sender of the
        packet. Note that this address may be changed in transit by a network
        address translation device;
    \item[Destination address, 32 bit] This field is the IPv4 address of the receiver
        of the packet. As with the source address, this may be changed in
        transit by a network address translation device;
    \item[Options, up to 288 bit] Options are largely unused, and may be considered harmful by
        some router. This is the portion of the IP header that is variable in
        size.
\end{description}


\begin{figure}[h]
    \centering
    \includegraphics[ width=1.0\linewidth, height=\textheight, keepaspectratio]{./pics/tcp/tcpHeader.png}
    \caption{Header of a TCP segment.}
    \label{fig:tcpHeader}
\end{figure}

On the TCP header, instead, Figure~\ref{fig:tcpHeader} shows the header
structure,

\begin{description}
    \item[Source port, 16 bit] Identifies the sending port;
    \item[Destination port, 16 bit] Identifies the receiving port;
    \item[Sequence number, 32 bit] Will be covered later, its role is to keep
        track of the sequence of segments;
    \item[Acknowledgment number, 32 bit] Same as the above, but keeps track
        of the \emph{successfully delivered} packages;
    \item[Data offset, 4 bit] Specifies the size of the TCP header in 32-bit
        words. The minimum size header is 5 words and the maximum is 15 words
        thus giving the minimum size of 20 bytes and maximum of 60 bytes,
        allowing for up to 40 bytes of options in the header. This field gets
        its name from the fact that it is also the offset from the start of the
        TCP segment to the actual data;
    \item[Reserved, 3 bit] For future use and should be set to zero;
    \item[Flags, 9 bit] Contains $9$ flags, with some important ones:
        \begin{description}
            \item[URG] Indicates that the Urgent pointer field is significant;
            \item[ACK] Indicates that the Acknowledgement field is significant. All
                packets after the initial SYN packet sent by the client should have
                this flag set;
            \item[PSH] Push function. Asks to push the buffered data to the
                receiving application;
            \item[RST] Resets the connection;
            \item[SYN] \emph{Synchronize sequence numbers}. Only the first packet sent
                from each end should have this flag set. Some other flags and
                fields change meaning based on this flag, and some are only valid
                when it is set, and others when it is clear;
            \item[FIN] Last packet from sender.
        \end{description}
    \item[Window size, 16 bit] The size of the \emph{receiving window}. It is
        the number of window size units that the sender of this very segment is
        willing to receive. Useful in flow control;
    \item[Checksum, 16 bit] Checksum for error\--checking;
    \item[Urgent pointer, 16 bit] If the URG flag is set, then this 16-bit
        field is an offset from the sequence number indicating the last urgent
        data byte;
    \item[Options, variable 0–320 bit, in units of 32 bit] Various options.
\end{description}

\section{TCP Architecture}

\subsection{The unpredictable and unrepeatable nature of TCP}

TCP has many different implementations, depending mostly on OS of choice.
Several variants of its components have been written, with many of them largely
optional. TCP works in segments. Execution flow at application level works
independently and unpredictably with respect to the TCP-level flow: when an
application sends something, multiple TCP packages are exchanged \-- how many
and when they are sent is not predictable. 

The sequence of bytes is first copied to a buffer (sliding window) and the
\texttt{send()} function is, for example, invoked \--- the exact time in which a
segment is sent is \textbf{unpredictable}, \textbf{unrepeatable} and depends on
many things, over which the application has little to no control. This means
that application cannot \emph{deterministically predict} the exact time in
which the transmission will occur \--- instead, the TCP layer will deliver the
payload at a specific time determined by the protocol\footnote{However, also on
TCP there is a level of unpredictability. Many more effects can concur: for
instance, the Operating System may not be ready to send a segment, or may be
busy with other resources. These situation will cause delay in sending
information, even from the TCP layer's point of view.}.

On TCP, various \emph{events} can induce a transmission of data:

\begin{itemize}
    \item application invokes \texttt{send()} \--- in this case, the data in
        send's call argument will be inserted into the proper transmission
        buffer, and it will be transmitted somewhere in the (possibly
        immediate) future; 
    \item application invokes \texttt{receive()} \--- this way, we will see in
        detail that the TCP protocol will send a \emph{response} that
        \emph{acknowledges} the receipt of a segment, for the other party's
        sake;
	\item TCP layer \emph{receives a segment} \--- the same as above;
    \item a \emph{timeout} occurs \--- TCP layer will inform the other party of
        this;
\end{itemize}

Each of the above will trigger a transmission either immediately or
\emph{withing a maximum predefined time} (in the case of a timeout, for
instance). When a TCP layer ``is touched'' from above or below, \textbf{it reacts
by transmitting a segment}. Transmission should occur \emph{even if there is no
useful data to transmit} (e.g. if the transmission buffer is empty) \-- in that
case, a segment will only carry the header, which has information useful to the
protocol itself. Basically, regardless of the presence of data in the
transmission buffer the TCP protocol will exchange vital protocol information.

Upon transmission, TCP may deliver a varying number of bytes, ranging from
empty up to a number of bytes \emph{larger than Maximum Segment Size} ($536$ in
Internet network, $1460$ for same Ethernet network). Since huge payloads cannot
fit into a single segment with a predefined MSS, the TCP protocol
\emph{fragments} it before delivery, resulting in \emph{multiple segments
delivered in sequence}. TCP protocol will try its best to send as many segments
as possible, for efficiency's sake: it is not uncommon that $2$ or $3$ segments
are initially delivered before receiving the acknowledgement for them.

\begin{table}[ht]
\centering
\begin{tabular}{cc}
CPU Cycle & $0.3 ns$ \\
Main Memory Access (DRAM) & $120ns$ \\
SSD & 50-150 $\mu s$ \\
HHD & $10 ms$ \\
Internet, San Francisco to New York & $40ms$
\end{tabular}
\caption{Some interesting metrics. Notice the order of magnitudes differences
between CPU cycles and network delays. This table highlights the fact that
bytes cannot be injected through the internetwork at full\--speed: suppose one
has a $100MB$ transmission buffer that is delivered across just $1ms$ \--
injected throughput would be $100 \cdot 10^6 \cdot 8 b/(10^{-3} s)$, which
yields an astonishing $8 \cdot 10^{11} b/s = 800 Gb/s$, unsustainable for most
of internetworks.}
\label{tab:SomeMetrics}
\end{table}
\bigskip

The TCP protocol starts with \emph{slowly sending segments, increasing the exchange
speed as the time goes on}. Acceleration mainly depends on the timing of
\emph{received} segments, by looking at the metrics of confirmation packets
from receiver. As the sender acknowledges that the receiver is able to keep up
with the increasing transmission speed, it raises up the delivery speed. Basically, TCP layer adapts its speed to $2$ determining factors,
\begin{itemize}
    \item the receiver's speed at processing packages;
    \item the internetwork's capability of deliver packages.
\end{itemize}

As we will see in following chapters, the first factor is handled by the
\textbf{Flow Control} algorithm, while the second one is kept under control by
the \textbf{Congestion Control} system.

\subsection{TCP transmission: the \texttt{send()} system call}

The system call \texttt{send()} processes the data transmission through the
network. The \texttt{send} function passes a memory buffer contained in
application space (address, length), and copies bytes from transmission memory
buffer in application space to transmission memory buffer in TCP layer (called
\textbf{TX-buffer}).

\begin{lstlisting}
public void write(byte[] b)
	throws IOException
\end{lstlisting}

Send is first invoked by application level, whose execution flow is completely
independent from the TCP layer's one. Buffer at application layer is then
copied to the TCP transmission buffer, to be sent immediately or later. New
invokations of \texttt{send()} will copy data in TCP buffer \textbf{after} the
data that is already present.

Invoking \texttt{send()} \textbf{does not guarantee} any immediate transmission, all it
does is pushing application data into TX-buffer to be sent in future. TCP will
then independently establish the proper moment and way to transmit data present
in transmission buffer.

Suppose to send $N$ bytes with $K$ consecutive \texttt{send()} invocations. How
many segments will be exchanged? How many transmission events?

The number of segments will not depend on $K$, since no matter how many times
$send()$ is invoked, the end result will be to insert $N$ bytes in the TX-buffer.
As a first approximation, the number of transmitted segments will roughly be
$$\mbox{\#transmitted } = \frac{N}{MSS} + 1,$$ with the last segment $+1$
smaller than the previous ones. Things, however, can be much more complex due
to packet loss and retransmissions. Recall that the number is not predictable
and not repeatable.

\subsection{Receiving data: the \texttt{receive()} system call}

System call \texttt{receive()} is quite similar to the \texttt{send()} call:
both receiver's TCP layer and Application layer execution flows act
independently to each other, and receiving buffer is not guaranteed to contain
any data.

When data reaches the receiver, the data is copied into the receiving buffer
(\textbf{RX-buffer}). The receiving buffer stores all received bytes and is flushed only
when the application invokes \texttt{receive()}. The function
\texttt{receive()} copies (at least a portion of) the receiver buffer into the
application buffer by means of passing a buffer with known \emph{address} and
\emph{length}. Function \texttt{receive()} copies bytes without exceeding the size of
the buffer (\emph{length}) and it returns how many bytes are copied into the
provided buffer. As argument, \texttt{receive()} also takes the number of bytes
to fetch from the TCP receiving buffer. Basically, \texttt{receive()} must
receive a buffer to which data should be copied, its length (in C length
information is \textbf{not} intrinsic to arrays) and the expected number of
bytes to be retrieved.

There are three possible outcomes for the \texttt{receive()} call:

\begin{itemize}
    \item the \emph{receive buffer is empty}: the application is suspended and
		the process is put to sleep until some data is available;
    \item \emph{more than \texttt{length} bytes are available}: a
        \texttt{length} number of bytes is fetched and delivered to the
        application as soon as possible. More \texttt{receive()} calls are
        needed to retrieve all data and empty the RX-buffer;
    \item \emph{less than \texttt{length} bytes are available}: all available
        bytes are copied as soon as possible, since they are less than the
        maximum deliverable value. The single \texttt{receive()} call will not
        yield all requested bytes, and more calls should be used.
\end{itemize}

MARK
\section{Sequence numbers}

IP protocol is \emph{unreliable} (packets can be lost, duplicated, or delivered
in different order from which they were sent), each data byte is implicitly
identified by a $32$ bit \textbf{sequence number}. The association is implicit
\--- the sender applies a sequence number to a segment, and the receiver uses
that information to reconstruct the original order of segments. Segments are
then reconstructed, and the data assembled as it originally was.

There are several sequence numbers. \emph{snd.User} is the variable carrying
the value of the next byte the \textbf{application} will send.
\emph{snd.Next} is the variable carrying the value of the next byte that the
\textbf{TCP layer} will transmit \--- its value is contained in the TCP header
(initial byte of the sequence, of course). Basically, \emph{snd.User} refers
to the next byte that will be filled by the invocation of \texttt{send()},
while \emph{snd.Next} is the next byte yet to transmit by the TCP layer. The
sequence number of application level must be computed from other information.
\footnote{An important detail is that \textbf{sequence numbers are 32-bit integers}.
Therefore, there may be a \emph{wrap-around} in an overflow-like fashion. TCP
should internally handle these comparisons in a proper manner.}

In short,

\begin{itemize}
	\item \emph{snd.Next} is the boundary between transmitted data and
		yet-to-transmit data;
	\item \emph{snd.User} is the boundary between in TX-buffer data (data
		sent by application) and not-yet-associated bytes.
\end{itemize}


\begin{figure}[b]
        \centering
        \includegraphics[ width=1.0\linewidth, height=\textheight, keepaspectratio]{./pics/tcp/TxBuffer.png}
	\caption{TX-Buffer and its flags \emph{snd.Ack}, \emph{snd.Next} and \emph{snd.User}.}
        \label{fig:TxBuffer}
\end{figure}

From the receiver's point of view, there is a variable, \emph{rcv.Next}, that
is the boundary between content of RX-buffer and not-yet-received data (right
boundary of the data currently in buffer). The variable \emph{rcv.Next} is
similar to the variable \emph{rcv.Next} in the sense that it points at the
next byte yet to receive. Only packets having expected sequence number are
collected and put in the buffer. There is the case where some packet has new
parts of information and sequence numbers not collected: in that case the
incoming segment will still be collected, with duplicated bytes thrown away.
There may be two reasons for packet duplications: IP duplication, and TCP
sender retransmission because it thought it was lost. TCP stores packets in
buffer until acknowledgement has been received, since they could be
retransmitted in the immediate future.

\begin{itemize}
	\item \emph{rcv.Next} points at the next byte that is not yet being
		received;
	\item \emph{rcv.User} points to the next byte to be received by the
		application. After \texttt{receive()} invokation by the
		application, all delivered to the application bytes can now be
		deleted, since there are no retransmission needs. 
\end{itemize}

\begin{figure}[b]
        \centering
        \includegraphics[ width=1.0\linewidth, height=\textheight, keepaspectratio]{./pics/tcp/RxBuffer.png}
    \caption{RX-buffer and its flags. Notice that RX-buffer only uses two flags
    instead of three (no ACK flag needed).}
        \label{fig:RxBuffer}
\end{figure}




\section{Handling duplicates and loss}

IP is an unreliable protocol. This means that \emph{packets can be loss}.
Necessary mechanisms are

\begin{itemize}
	\item \textbf{retransmission};
	\item \textbf{acknowledgement}, which is a kind of \emph{notification
		of receipt}, in order to be sure that the receiver has received
		all the data we sent them.
\end{itemize}

Acknowledgements are a mechanism that assures receipt of a message by
notifications. Every header of a segment contains the sequence number of
\emph{snd.Next}. Every segment also \emph{carries an information regarding
the bytes that are received}: the \textbf{acknowledgement number}, which tracks
the state of the RX-buffer by the pointer \emph{rcv.Next}. This way, having
both sequence number and acknowledgement number, one can successfully track the
state of a TCP connection. Upon sending a TCP segment, both sequence number and
acknowledgement number are sent, so that the receiver can reconstruct the state
of the sender RX-buffer.

A fifth variable is needed: \emph{snd.Ack}, which \emph{points to the byte in
TX-buffer that are both transmitted and acknowledged}. This variable is only
increased upon receiving data (for instance, upon receiving a sequence with a
greater ACK number from the sender). Data before \emph{snd.Ack} pointer can
safely be discarded. Acknowledgements are crucial to a TCP connection, in order
to guarantee \textbf{reliability} of a connection (TCP is connection-oriented).
Therefore, transmission is always necessary even if TX-buffer is empty. That
case, no payload will be transmitted, only information in header is sent
(increasing acknowledgement number). 

Data bytes between pointers \emph{snd.Ack} and \emph{snd.Next} is said to
be \emph{in flight} data. These bytes have been transmitted but not yet
acknowledged.


\section{Delayed Acknowledgement}

\emph{Delayed Acknowledgement} is a famous TCP algorithm. It is pretty
straightforward:

\begin{quote}
Upon receiving a segment, if delayed-ack timer $T$ has previously been set,
transmit immediately. Else, set the delayed-ack timer $T$ to a value.
\end{quote}

Delayed-ack timer value depends on the operating system:

\begin{itemize}
	\item RFC suggests $T = 500ms$;
	\item Windows has $T = 200ms$;
	\item Linux in general has $T = 40ms$;
	\item RHEL sets $T = 4ms$.
\end{itemize}

If there is not much data to transmit, it will be likely that the timer $T$
will expire. If the other part is sending a lot of data, the contrary will be
more likely to occur, breaking the awaiting. \emph{In order to help the other
part, acknowledges will be delivered as soon as a second segment is received}.
The core idea is to both help the other end and minimize the number of segments
to be sent. In fact, a delay time $T$ assured to save sending some segments, a
feature that historically was of a crucial importance.

\section{Retransmissions}

The connection state includes three more variables:

\begin{itemize}
	\item a \textbf{retransmission timer};
	\item a \emph{variable that describes the duration of retransmission
		timer}, the \textbf{RTO};
	\item a \textbf{retransmission counter}.
\end{itemize}

The algorithm is as follows:

\begin{quote}
Upon transmitting segment S, the counter is cleared and set to $0$, with the
timer set to the RTO value. Upon receiving an ACK, \emph{snd.Ack} is set to
the maximum value between \emph{snd.Ack} and \emph{segment.Ack}. If
\emph{snd.Ack == snd.Next} the timer is switched off.

When timer expires, the counter is incremented and if the counter has not yet
reached a \emph{MAX\_COUNT} value, the segment is retransmitted. However, this
time the timer value is set to RTO but \emph{RTO = 2 * RTO}. Only in-flight data
should be retransmitted (and all of it after timer expires). Basically, data
for which we are sure that it has been received, should not be retransmitted in
case the timer expires.

When counter reached \emph{MAX\_COUNT} value, connection is closed.

A particular condition is when an ACK arrives for a portion of the data that
has been sent. In that case, the timer resets (the receiver has responded) and
the sender awaits ACK for missing segments.
\end{quote}

Windows closes connection after $5$ failed attempts, Linux after $15$. It is
simple to realise that there may be a lot of unnecessary retransmissions. The
timer can, for instance, run out too soon for the acknowledgement to reach the
sender. Unnecessary retransmissions are a waste of resources, a fundamental
problem. The reason could be one of those:

\begin{itemize}
    \item segments are \emph{lost};
    \item segments \emph{which ACK are lost};
    \item RTO was set to a \emph{too small value}.
\end{itemize}

Thus, RTO should be set to an appropriate value, since a too short value leads
to high overhead and possibly many unnecessary retransmissions, while a too
long value results in high latency and possibly a slow connection. RTO value is
basically a trade-off between few retransmissions and fast connection. RTO
should be set \textbf{dinamically}. The RTO should be slightly greater than the
\emph{RTT} (round-trip-time), and it is an idea from Jacobson algorithm. This
is of a crucial importance for TCP to work. Initial RTO value is heuristical,
and varies from one OS to another. Linux and Window start from same value,
macOS use a different value, and so on.

\section{Multiple default gateways}

More default gateways could be added to a single node. Reasons to add more than
one default gateway all boil up to \emph{failure avoidance}. To know whether a
gateway has stopped working, a heuristic TCP algorithm tries to detect a gateway failure:

\begin{quote}
	if the number of retransmissions is greater than a \emph{MAX\_COUNT}
	divided by 2 number, the \emph{connection} changes its default gateway.
	Moreover, if the number of connections that changed default gateway is
	greater than the number of open connections divided by 4, the \emph{IP
	layer} changes the default gateway. This last feature speeds up
	reconfiguration of early connections that still have to make some
	retransmission attemps.
\end{quote}

Basically, each connection can autonomously choose its own gateway, but the IP
layer is able to force any \--- new or already present \--- connection to use a
different default gateway.

\section{Gaps in RX-buffer}

Suppose that 4 segments are sent, and the second one has been lost. In this
case, the receiver received all segments except the second one, however it has
no method to inform the sender to retransmit only the second segment. Sending
ACK for only the first segment would result in unnecessary retransmission of
segments 3 and 4, which were correctly received. The receive buffer could end
up having some \textbf{gaps}, missing bytes that are supposed to be received.
The solutions are \emph{Selective Acknowledgements} (SACK), a special kind of
acknowledgements that carry both \emph{left and right sequence number edges} of
each out-of-order \textbf{block} in RX-buffer, this way avoiding unnecessary
retransmissions. SACK protocol must be supported by both members of a
connection. SACK is very convenient, since many segments can be lost when a
sender tries to deliver dozens of segments at once. This way, TCP can achieve
\emph{efficient handling} of the connection.

Of course, out-of-order segments could still lead to gaps in RX-buffer. In this
case, unnecessary retransmissions are unavoidable when an out-of-order segment
reaches the receiver too late (however, SACK reduces a lot the burden to the
sender since only missing segments should be selectively sent).

\section{Operating System TCP interrupts}

Upon packet arrival, a \emph{system interrupt} is sent. 

\chapter{Flow and Congestion Control}

Whenever TCP decides to transmit, there are three possible scenarios: 
\begin{enumerate}
	\item an empty segment is sent when there is no payload;
	\item it transmits a single segment if payload data is smaller than MSS;
	\item it transmits multiple segments if payload size exceeds MSS.
\end{enumerate}

TCP initially starts slowly, then \emph{increases its transfer speed} according
to the rate in which acknowledgements are received. The overall interaction is
bidirectional and quite complicated, since TCP implementation at sender's side
tries to adapt to both connection properties and receiver's side properties.
As a general rule, the number of in-flight bytes is always lower than an upper
bound that is dynamically updated \--- since the upper bound is chosen
according to proper criteria, TCP can \emph{adapt} to the peculiar
characteristics of the connection and receiver's speed. Naturally speaking, a
sender should not send more segments than how many can be managed by the
receiver \--- \emph{how many} is an upper limit that dynamically adapts.

There are two different algorithms that will set two different bounds: the
\textbf{flow control} and the \textbf{congestion control}. The TCP layer will
always send less data than the minimum between the two upper bounds. The first
algorithm constructs a bound in such a way that the capacity of the receiver is
always respected. Let an extremely fast computer send data faster than a
receiving, slow, computer. Slower computer has not enough speed to collect all
data that has been sent \--- the flow control algorithm lets the sender speed
adapt to the receiver speed by means of a \emph{send window}. The second
algorithm, the congestion control, constructs a state variable called
\emph{congestion window}, whose goal is to model the maximum current throughput
of the network. The number of in-flight bytes \emph{must be slower than both
send window and congestion window multiplied by MSS}, so that
$$\mbox{in-flight} \leq \min(\mbox{sndWin}, \mbox{congWin} \cdot \mbox{MSS}).$$
Basically, if the transmission buffer is full of data, a number of in-flight
bytes equal to the minumum of both quantities should be sent, otherwise just
send \emph{snd.User - snd.Ack} bytes (those still to send).

\section{Managing memory buffers}

Buffers cannot increase indefinitely: boundaries must be set in order to assure
system stability. In Linux kernel, buffer size are set upon compilation and are
fixed in size. In case data cannot be pushed into the TX-buffer anymore because
it is full, application should be suspended. When the RX-buffer is full,
packets have to be discarded since they cannot be collected. Flow control will
act to prevent this situation by letting the sender know how much free space is
available to the RX-buffer.
\section{Flow control}

The goal of flow control is to keep a fast transmitter from overrunning a
slower receiver.

Since application sends much data and faster than ACK arrive, the TX-buffer
could end up being filled up. At receiver's side, application invokes
\texttt{receive()} much slower than incoming packages speed. This way, the
RX-buffer could fill up as well. To solve this issues, application invokes
\texttt{send()} only when TX-buffer is full, hence it is put to sleep (blocked)
until TCP gets proper ACK and advances \emph{snd.Ack} (when it has more free
space). TCP sender keeps track of free space in other end's RX-buffer by
looking at a variable in header that informs it of how much free space is
available, so that it can predict how much free space there is. When it guesses
that receiver's RX-buffer could be full and have no space available, it stops
transmission and suspends the application. If the maximum window corresponds to
the size of a single segment, the protocol is called \textbf{stop-and-wait}.
Larger windows enable pipelining of multiple segments in a row, enabling far
more efficient usage of the connection.

Flow control algorithm adopts the concept of \textbf{sliding window}. Each
segment header has a \emph{WindowSize} field in the header that contains
\emph{how many free bytes are in the RX-buffer}. WindowSize field basically is
the amount of free space available in RX-buffer. At sending side,
\emph{snd.winSize} contains the \emph{number of free bytes in RX-buffer of
the other side}. Initially it is set to the receiver's buffer size, and it is
updated dynamically upon sending a segment. If \emph{S.Ack} is greater or
equal to \emph{snd.Ack}, then the variable \emph{snd.winSize} is set to
\emph{S.windowSize}. Basically, the number of in-flight bytes must never
exceed the number of bytes in \emph{snd.winSize} that are available in the
RX-buffer. The goal of TCP is to reach a number of in-flight bytes that is as
close as possible to the \emph{snd.winSize} number of bytes, in order to
optimize the connection efficiently.

\begin{figure}[b]
    \centering
    \includegraphics[ width=1.0\linewidth, height=\textheight, keepaspectratio]{./pics/tcp/idealThroughput.png}
    \caption{Ideal throughput condition, in which the injected throughput is
    equal to M $\cdot$ MSS at each time $T$. In scheme, every timing is
considered constant, and the total time $T$ is the sum of the time in which all
the segments arrive at the receiver's end, the receiver's processing time and
the acknowledgement's time to arrive at the original sender's side.}
    \label{fig:idealThroughput}
\end{figure}

\clearpage


\section{Performance estimation}

From flow control, let's determine the maximum T-IN-TCP that can be obtained.
TCP is injecting some throughput into the network, and the amount of it is
controlled by the flow control algorithm. 

The best case one can get by flow control is $M * MSS$ every RTT \--- this is
the case where the receiver's application empties the RX-buffer the moment the
segments arrive. In the case the application empties \emph{half} the buffer at
periodic intervals, the throughput would be the half as well, $M/2 * MSS$ every
RTT. If the entire buffer is emptied but with a delay or not aligned with the
receipt of segments, the throughput would be $M * MSS$ every RTT + $\Delta$
(more time than plain RTT). Combining all cases, one could get less than $M *
MSS$ data every \emph{longer} than RTT, depending on the parameters and the
circumstances of the network.

For the best case, it's important to recall that some important assumptions
have been made:
\begin{itemize}
    \item all intervals have same time duration $T$, as described in
        Figure~\ref{fig:idealThroughput};
    \item all the segments arrive together, or at least not after a time which
        is irrelevant or already counted in $T$.;
    \item acknowledgement arrives immediately, and not after a huge delay.
\end{itemize}

In every case, a greater RX-buffer will result in higher throughput, since
there is a proportionality. Viceversa, greater RTT will result in less
throughput \--- LANs will have greater throughput than WANs. However, in this
formula quality of the network is not taken in account: segments can be loss,
especially when too many of them are injected through the internet. For this
reason, this much-optimistic model cannot be applied in cases where the
throughput is high (read \--- when RX-buffer is insanely huge). Every time a
retransmission is needed, throughput becomes much smaller than the ideal case,
and this phenomenon is much heavier as the RTT increases, since retransmissions
are harder to assess.

Another thing to consider is maximum network speed. Injected throughput could
be \emph{even greater} than maximum network speed, in that case many segments
would be loss, resulting in a sub-optimal efficiency\footnote{This is solved by
Congestion control mechanism.}.

\subsection{Establishing size of RX-buffer}

Size of RX-buffer should have the following characteristics:
\begin{itemize}
    \item the size should be \emph{a multiple of MSS} \--- this is done to
        avoid leaving unused space at the end of the buffer, since in the lucky
        case exactly a multiple of MSS will be delivered;
    \item the RX-buffer size is a \emph{trade-off} between greater ideal
        throughput and lower throughput due to excessive retransmissions, and
        should be chosen accordingly;
    \item usually, $64KB$ are chosen; the value is OS-dependent. The size is
        managed as if it were a multiple of MSS \--- when almost full, pretend
        it is full and ignore last portion of buffer.
\end{itemize}

\begin{table}[ht]
\centering
\begin{tabular}{ccc}
    Flow control & Ethernet & Internet \\
    MSS  & $1460B$ & $536B$ \\
    RX-buffer & $64KB$ & $64KB$ \\
    number of segments in-flight & $44$ & $122$
\end{tabular}
\caption{Quantities involved in flow control, on Ethernet and Internet.}\label{tab:FlowControlQuantities}
\end{table}
\bigskip

\section{Sustainable throughput}

A sustainable throughput is not potentially infinite. After a certain injected
throughput \emph{T\_MAX}, the network bottlenecks and starts losing segments
\--- this behavior will certainly lead to less overall performance. Thus, an
optimal throughput should be determined and largely depends on connection
quality. Basically, injected segments are delivered at same speed only if the
injected throughput is lower than the maximum possible throughput,
\emph{T\_MAX}.

\begin{figure}[b]
    \centering
    \includegraphics[ width=0.6\linewidth, height=\textheight, keepaspectratio]{./pics/tcp/networkCollapse.png}
    \caption{Network collapse after injecting a too-high throughput. Notice
        how injecting an even greater throughput will result in a \emph{lower}
        throughput.}
    \label{fig:networkCollapse}
\end{figure}



An excessive throughput \textbf{will} result in the following consequences:
\begin{itemize}
    \item packets exit \textbf{slower} than they entered;
    \item network's max sustainable throughput \emph{T\_MAX} will
        \emph{collapse} \--- this fenomenon is called \textbf{network
        congestion};
    \item network \textbf{will need some time to remove its congestion},
        with even worse behavior in the case the unsustainable throughput is
        maintained for a longer time.
\end{itemize}

Congestion issues happen mostly because networks have different speeds, and at
lower level this cannot be perceived \--- it is up to the TCP layer to detect
issues during path, and slow down segments injection. Routers commonly have a
mechanism for packet queuing, which can be filled up, resulting in packet
discarding. Such discarded packets will need a retransmission, leading in lower
performance. New segments and packets will increase the time for which the
queue is dissolved.

\section{Congestion control}

Suppose the bound of flow control is so large that congestion control is
predominant (snd.congWin*MSS < snd.WinSize). A congestion control algorithm
could try do infer the maximum throughput by looking at the RTT and calculating
MSS/RTT < T\_MAX. However, this is unfeasible, since it requires too much data.
A second issue is that the quantity \emph{T\_MAX} \emph{varies during time}:
it depends on nominal throughput of networks and routers, and on
\textbf{competition} of different injected throughputs in the network.
Basically, the competition is \emph{unpredictable} and \emph{time-varying}.

\subsection{Actual algorithm}

Congestion control algorithm have some important components,
\begin{enumerate}
	\item slow start;
    \item congestion avoidance;
    \item fast retransmit;
    \item fast recovery;
\end{enumerate}

each one with several variants and many parameters to be properly set. We will
see \emph{Tahoe} algorithm, but the most used one is the \emph{Reno}
algorithm.

The state variable \emph{CongestionWindow(t)} is set to an initial very small
value. Its value will increase whenever Acknowledgements are received in time
\--- the network is assumed to be not congested. Upon retransmission (timeout
expiration) the state variable is decreased a lot, and the network is assumed
to be congested. Another state variable, called \emph{SlowStartThreshold}, is
a threshold under which \emph{CongestionWindow(t)} increases very fast, while
below it the same state variable increases slowly. Initial values of
\emph{snd.congWin} and \emph{snd.ssThresh} are, respectively, (usually) $3$ MSS
and $64KB$. Whenever an acknowledgement is received in time, if the congestion
window is lower than the threshold, increase fast with \emph{slow start
algorithm}; otherwise, increase slowly with \emph{congestion avoidance}
algorithm. Upon retransmission (timeout expiration), the congestion window is
reset to MSS and the threshold is reset to the maximum value between $2$ MSS
and (snd.Next - snd.Ack)/2 \--- that is, the maximum value between the double
of MSS and the half of the in-flight bytes. The assumption upon retransmission
is that the network is congested: this behavior is hugely inefficient, since
\emph{many} concurrent causes may determine a packet loss \--- for instance,
small RTO, network loss (e.g. WiFi). The congestion window is increased by +MSS
whenever an in-time ACK is received. Congestion avoidance algorithm does the
same, but only upon receipt of \textbf{the last} in-time ACK (only if
snd.congWin bytes are ACK'ed). 

The two algorithms increase the congestion window at \emph{much} different
speed. Slow start algorithm, despite its name, will \emph{double} the
congestion window each time an acknowledgement for all segments is received.
Congestion avoidance, instead, will increase the congestion window
\emph{linearly}, for which the window is increased by $1$ (single MSS) each
time an acknowledgement for all segments is received. The slow start will
provoke an exponential growth, while the congestion avoidance will provoke a
linear growth. 

\begin{figure}[hb]
    \centering
    \includegraphics[ width=0.7\linewidth, height=\textheight, keepaspectratio]{./pics/tcp/slowStartTypes.png}
    \caption{The two kinds of slow start evolutions. At left, exponential, at
    right, linear.}
    \label{fig:slowStartTypes}
\end{figure}



\subsection{Congestion control pattern in case of exponential growth}

Let's assume all segments are lost, and retransmission occurs after RTT (that
means, RTO is a good approximation of RTT). Let's also assume slow start
produces an exponential growth. Initial value of \emph{ssThresh\_start} is
$64KB$. Starting with threshold value below \emph{T\_MAX}, slow start will
yield an exponential growth; after some time, congestion avoidance will enter,
a segment loss will occur and the congestion window will be reset to half the
\emph{ssThresh\_start} original value. Initial threshold above \emph{T\_MAX}
will trigger a segment loss, and a reset of the threshold (without congestion
avoidance algorithm, that has not enough time to act). After the first segment
loss, threshold will be halved by setting it at half of the in-flight bytes,
and system will act as it started below the \emph{T\_MAX} throughput. The
``exponential, linear, drop'' pattern will occur \emph{forever}, and the three
phases (slow start, congestion avoidance, loss detection) will happen one after
the other.

\begin{figure}[hb]
    \centering
    \includegraphics[ width=1.0\linewidth, height=\textheight, keepaspectratio]{./pics/tcp/congestionControl.png}
    \caption{Congestion control and its dynamics. The pattern indefinitely
    repeats.}
    \label{fig:congestionControl}
\end{figure}


\clearpage

\subsection{Congestion control pattern in case of linear growth}

With same assumptions as above, but with slow start provoking a linear growth,
the pattern will be \emph{completely linear}, since no exponential growth
occurs. Basically, only the starting phase of each cycle will be different. The
end result in both patterns, however, is that the average \emph{T\_IN} value
will be \textbf{much} smaller than \emph{T\_MAX} \--- the TCP congestion
control cannot expoit internetwork capacity efficiently. Injected throughput
will always be lower than the maximum possible: the consequence, is that the
average throughput is lower than what the network could sustain, however, this
is very important in order to avoid network collapse and congestion.

\subsection{Fast retransmit in a nutshell}

Suppose $K$ segements are transmitted in a burst, and a single segment appears
to be lost. Resetting congWin to its minimum value is largely inefficient,
since only a single segment of the row has been lost. Another strategy could be
waiting for a timeout before resetting the congWin: the solution is inefficient
too, since waiting before retransmitting could lead to long breaks (how much to
wait? how to estimate a correct time interval?). The solution is to retransmit
\emph{immediately only} what appears to be the missing segment, and decrease
congWin \emph{but not} to its minimum value.

\subsection{Reasons of congestion control to slow down the network so much}

Intuitively, one would like to reach the maximum possible value for the
congestion window and the maximum throughput sustainable; however, they are
\textbf{unknown}. We can detect detect its value only after having injected an
excessive throughput, by detecing segment loss. An aggressive slow down occurs
at each detection of segment loss, in order to prevent the collapse of the
network. In fact, suppose many machines are injecting a throughput which is
closer to the maximum possible: in that case, if the network is congested,
\emph{it will require more time to return to its normal state}, because all
machines are insisting injecting throughputs close to unsustainable ones, and
removing congestion \emph{will} require larger time. Basically, this is a
heuristic way to avoid congestion on a network where multiple machines are
using transmission resources altogether.

In the end, congestion control strives to ensure \textbf{fairness}. Fairness is
the tendency of each competing TCP connection on a router to consume the same
fraction of the router capacity. Each connection, on the average, will consume
an equal fraction of router's capacity. However, today's standard impose
\textbf{node-level fairness}: this means that each node should have the same
amount of network capacity (thousands of TCP connections can be opened at the
same time from a single node). Note that UDP connections \emph{do not}
implement any flow or congestion control, and transmit at full-speed: they are
`selfish'.

\subsection{Estimating the average throughput}

Some assumptions should be made in order to compute average throughput:
\begin{itemize}
    \item slow start has \emph{linear growth};
    \item \emph{T\_MAX} is constant and in between $K\cdot MSS$ and$(K+1)\cdot MSS$;
    \item \emph{all} segments are lost at once (no wild retransmissions);
\end{itemize}

The average throughput that is injected is the average number of bytes in each
pattern, divided by $(K + 1)\cdot RTT$, that is FIXME
$$A_t = \frac{MSS\cdot \frac{K(K+1)}{2}}{(K+1)\cdot RTT} = \frac{MSS\cdot
K/2}{RTT}.$$ Thus, the injected throughput is the same one would obtain by
having a \emph{constant window} of $K/2 \cdot MSS$ size \--- on the average,
one can only get \textbf{half} of the maximum network throughput.

Slow start in exponential fashion makes this calculation quite more complex,
but we will genuinely assume a linear growth in all our calculations.

Another important scenario is not transmitting data after many RTTs. Without a
reset, one would use the last values that were determined by the congestion
control algorithm. In order to avoid bad performance, the sender \textbf{resets
both congWin and ssThresh} after a timeout. Basically, the approach is very
pessimistic and conservative, in order to avoid any kind of congestion in the
network.

\section{Interaction between flow control and congestion control}

The interaction between the two TCP control systems is not straightforward. At
connection opening, the bound imposed by congestion control is \emph{much}
tighter than the flow control one (2-4 MSS instead of $64KB$). Hence, at the
beginning the network will follow congestion control's dictated bounds \---
data arrives slowly.

After connection opening (steady-state), there are two possible cases:
\begin{itemize}
    \item T-MAX > T-FLOW-CONTROL \--- maximum network throughput is greater
        than maximum flow control throughput: in this case, the bottleneck is
        the \textbf{receiver's capacity}. The steady state throughput will be
        $\frac{M\cdot MSS}{RTT}$, while its connection opening transient
        throughput will be half of that value;
    \item T-MAX < T-FLOW-CONTROL \--- maximum network throughput is lower
        than maximum flow control throughput: in this other case, the
        bottleneck is the \textbf{internetwork}, and congestion window will go
        up and down, never reaching the value corresponding to the receiver's
        buffer size. The steady state throughput will be $\frac{K\cdot
        MSS}{2MSS}$.
\end{itemize}

Many variants of Congestion control have been developed. In particolar, there
are many families for different use-cases:
\begin{description}
    \item[congestion collapse] this class attempts to increase the average T-IN
        upon congestion collapse in a standard, wired internetwork connection;
    \item[wireless] this class strives to optimize correct non-wired environments, with algorithms tailored to environments with high network loss;
    \item [high-speed] this class is tailored to sustain very-high bandwidth
        connections (e.g. backbones), with \emph{high bandwidth-delay product}.
\end{description}


The latter is the case of backbones, with very-high bandwidth-delay product
\--- those are all the networks that, if TCP is left as default, will lead to
several minutes of connection start and gigabytes of data exchanged for the
sole goal of bringing the connection to full-speed. In order to optimize them,
several variants of TCP have been developed.

Suppose a sustainable throughput is $T_{MAX} = K \cdot MSS / RTT$, and suppose
the length of the first initialization period be $t_{init} = K \cdot RTT$.
Solving in $K$ leads to $$t_{init} = \frac{T_{MAX} \cdot RTT^2}{MSS}.$$ By
looking at the formula, one recognizes that the initial time has a squared
dependency over round-trip time, a direct proportionality over $T_{MAX}$ and is
inversely proportional to MSS. In all connections where both RTT and $T_{MAX}$
are large or extremely large, the initialization time could become relevant as
well as the sheer quantity of the data that is needed (more than $3$ minutes of
transient, more than 100GB transmitted).


\chapter{Connection opening}

\section{Sequence numbers initialization}

Upon connection opening, since they are not set to zero, each of the two sides
should agree to their initial sequence numbers. The ideal starting point is
that every \emph{snd} pointer should be set to the same value, as well as the
other party's \emph{rcv} buffer. A first idea could be to simply send initial
segments in which \emph{snd.Next} are specified in header. Special segments
carry a flag that denotes a connection request (from the client) and `ok' (from
the server). Both parties will set their initial \emph{rcv} pointers to the
value read in the header from the other party. In reality, this first
implementation has many issues:
\begin{itemize}
    \item delayed duplicates from client may be received after a long time \---
        the server might uncorrectly believe another connection request is
        coming from the client (since there are port numbers, however, the new
        connection will be opened only in case the previous one has been
        closed);
    \item delayed duplicates from server may be received after a long time \---
        this way, an opening with wrong sequence numbers may occur.
\end{itemize}

The solution to the above problems, in which delayed packets may be
undistinguishable from not delayed ones: the ``\emph{cross your finigers}''
approach. The machines \emph{assume} that a predefined, maximum segment
lifetime exist \--- segments cannot exist after this supposed lifetime. Since
the assumption does not match reality, the algorithm execution does not provide
any guarantees. The lifetime in question is named \textbf{Maximum Segment
Lifetime (MSL)}, and it is arbitrarily set to $2 min$ ($120s$). After such MSL
time passed, the machine assumes that the segment will never reach it. This is,
in reality, wrong: IP do not have concept of time, and packets are discarded
only based off their number of \emph{hops}. 

\subsection{The three-way handshake protocol}

The sequence numbers are generated by the \textbf{ISN-generator} (initial
sequence number generator), a $32$-bit counter increased every $4\mu s$, even
when the node is switched off. Overflow will happen every 4-5 hours. The
generator is used to generate an initial sequence number from its current
value. A sequence number can used again at least after $4$ hours.

\begin{figure}[b]
    \centering
    \includegraphics[ width=1.0\linewidth, height=\textheight, keepaspectratio]{./pics/tcp/threeWayHandshake.png}
    \caption{The three-way handshake.}
    \label{fig:threeWayHandshake}
\end{figure}



The three-way handshake protocol (Figure~\ref{fig:threeWayHandshake} will begin
with sequence number initialization, from the ISN-generator (suppose it's $x$),
and send a SYN(seq=x)\footnote{SYN bit is a flag in TCP header that is always
zero, except in the first and second segments, and it is used only for
requesting connections.} packet to the server. The latter will answer with a
package such as SYN(SEQ=y, ACK=x+1). The client will then respond with a second
message, containing data and having sequence and acknowledgement numbers
(SEQ=x+1, ACK=y+1); now, connection has been established and both parties will
assume the connection is successfully opened. The client side will be sure that
no delayed duplicates are coming since $x$ should have come from a connection
which has been opened more than $4$ hours ago! The same reasoning happens from
the server side: any delayed duplicate should have been traveling for more than
$4$ hours. This protocol even protects in the case old duplicates are received:
that case, the other party will receive a wrong packet \--- for something they
did not request at all \--- and respond with a REJECT packet\footnote{RST is a
header flag as well as SYN, and it is used only to reject connections.}. The
client will respond with a REJECT(ACK=y).

A very unlucky case is where the server receives two delayed duplicates,
however, there is little to no chance that the ACK values are correct (and, in
that case, it would have traveled for more than $4$ hours, much more than MSL
value of 2 minutes). The same goes if the server receives a REJECT (it cannot
be a duplicate). The mechanism is shown in Figure~\ref{fig:threeWHduplicates}.

\begin{figure}[b]
    \centering
    \includegraphics[ width=1.0\linewidth, height=\textheight, keepaspectratio]{./pics/tcp/threeWHduplicates.png}
    \caption{Possible errors in TCP handshake. The TCP protocol and its
    sequence number system is designed to avoid unsolicited connection opening,
preventing the server to waste resources with no reason.}
    \label{fig:threeWHduplicates}
\end{figure}



RST segments are send in two cases,
\begin{itemize}
	\item unexpected rcv-seqnum in 3-way handshake;
    \item received segment for closed connection;
\end{itemize}
in those cases, send a RST segment with seqnum expected by peer.

In case the RST packet is legit (that is, when it carries the sequence number
the machine expects), then \textbf{close} the connection and \textbf{ignore}
rcv-segment.

\section{Interface between 3-way handshake and application layer}

Application layer just requests opening a connection. The TCP layer will handle
the connection opening with appropriate system calls. The system calls in sequence are

\begin{itemize}
	\item socket;
    \item bind;
    \item listen(num,...);
    \item accept.
\end{itemize}

System call listen has a \emph{num} in argument that tells \emph{how many
connections can be queued}, that is, the number of incoming SYN segments that
should be queued. Until listen call, no connection can be opened and the server
will respond with RST. Up to \emph{num} SYN segments can be accepted at the
same time \emph{before} invocation of accept and delivery of SYN-ACK packages.
Closing a connection happens with FIN flag. It is handled the same way as SYN
is, and where one party wants to close connection, sets FIN flag to $1$ and the
other party will respond with a FIN flag to $1$ as well.

This protocol can be abused to block a server with little to no effort. Attacks
whose goal is to prevent the target working are called \textbf{Denial of
Service} attack. An example of DoS attack is the \textbf{SYN-flood} attack.
SYN-flood attacks are very hard to detect and very cheap to operate. The idea
is to keep sending SYN segments so that server queue \emph{is always full}: the
effect will be that legitimate SYN requests from legitimate clients will be
discarded. Some variants even hide the IP-src by modifying it, in order to hide
attack origin. SYN-ACKs responses will be delivered to (fake) IP-src address.
Randomly forging IP-src addresses makes almost impossible to filter them out
with a firewall. SYN-flood attacks are cheap: for $128$ opened connections
every $3$ minutes, only $128\cdot 40 = 5120B every 180s = 228 bps$ bytes are
needed to the attacker to forge necessary packets. To forge $1280$ connections,
$1280\cdot 40 = 51200B every 3 m = 6826bps.$ A small cost of sending a packet
causes the listener to force a listening to a connection, a \textbf{much}
higher cost. By designing a new protocol, one would likely avoid spending
resources upon the first SYN, by operating \emph{statelessly} until the
initiation can demonstrate its legitimacy.

\chapter{TCP Reliability}

TCP a \emph{reliable} protocol. Reliable only means that data are received, in
order, and with no duplicates. However, application layer do not know how many
bytes are received by the other side of the connection. \texttt{send()} may
complete with no error, but no byte could be transmitted \--- no way to know
how many packages are actually delivered to the other party. Upon an error
taken during the $n+1$-th send invocation, there is no way to know if previous
messages were actually delivered or not. Despite TCP being reliable, there is
\textbf{no guarantee} that messages up to $n$-th were delivered and
transmitted. FIXME-6

The correct reasoning is that all bytes \emph{up to} $k$ are correctly
delivered, but one cannot know exact $k$ value. Regarding bytes delivered to
the application level at the other end, bytes were received by TCP layer up to
$k$ but probably $k - k'$ bytes \emph{are still to be delivered to the
application}. Only a portion of them, $k'$, have been delivered. Both $k$ and
$k'$ are \emph{unknown}. Only after \emph{receive} call, with all ACK correctly
received, one is sure that all previous data have been successfully delivered.
Upon receive error, one \textbf{cannot tell anything about the previous send
invocation}, since there is no clue that the packet has been received correctly
or not. Any of these scenarios might be the reason:
\begin{itemize}
    \item \emph{not received}: packet never received by the partner;
    \item \emph{not delivered}: packet arrived, but partner crashed before
        reply;
    \item \emph{delivered}: but not reaching, connection broke.
\end{itemize}

All these scenarios must be handled differently and correctly. Response to
failure should always be a correct failure handling and proper recovery from
damage, no matter the kind of it. Typical examples of error handling are:
\begin{enumerate}
	\item take an error;
    \item repeat;
    \item wait some time;
    \item send request again;
    \item until receiving a response.
\end{enumerate}

However, this approach is wrong, especially when dealing with financial
transactions, where same requests, multiple times, can lead to catastrofic
errors and wrong operations. Only read operations can be handled this way
effectively.



\part{Threat model}

\chapter{Understanding threat model}

TCP offers no authentication. It means that if in DNS the address has been
maliciously altered, a connection can be opened to the attacker's service
rather than to the legitimate one. Network attacker may be in control of the
router \--- he may switch DNS request and respond with fake responses. TCP
offers no integrity either. A network attacker might change what the machine
sends and receives \--- a communication may be altered by an entity that acts
in between the parties. An improved version of TCP, \textbf{TLS}, is available.
It acts on top of TCP and uses cryptography to protect integrity and offer
authentication.

Suppose now the client has been infected by malware \--- attacker can control
the client's behavior from remote, for example modifying web pages, opening
connections, attempting priviledge escalation. In this case, TLS does not
guarantee secrecy, integrity, authentication. 

Basically, it depends on \textbf{threat model}. The threat model is the set of
actions that we assume the attacker can execute. Reasoning about security of a
system without a well-posed threat model makes no sense.

Typical threat models involve the following scenarios, in increasing pessimism:
\begin{itemize}
    \item \textbf{Network attacker} \--- attacker can only operate in the
        network. He is able to observe and communicate. DoS and MITM attacks
        are usually what a network attacker can do;
    \item \textbf{Compromised endpoint} \--- attacker has compromised a node in
        the network with a malware;
    \item \textbf{Physical access} \--- attacker has physical access to an
        endpoint;
    \item \textbf{Insider/Supply chain} \--- attacker has access to portion of
        the software supply chain, e.g. libraries, infrastructure.
\end{itemize}





\end{document}

